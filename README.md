
# ANLPPro

One possible idea that is evidenced from gpt thinking and gemini 2.5 pro is that the more the answer can be seen as a pattern that can be arrived at from brute force over tinier examples, and the less this is true, it seems like gpt can keep going in circles (Hamiid Hamiid Hamiid strong evidence also possibly pushing balls) (Where gpt actually got it after thinking was a couple of C's. The more obvious the problem the more either of the models get them as evidenced by the factual A)

More information, they dont parse memory related issues for problems probably because such problems where memory is something one should take care of isnt really seen often (2122 D). Also another thing to be measured is how important feeding samples are because both seem to take a pretty significant hit in accuracy without samples, which might not be expected as they are language models they should be able to understand without. Also even though gemini does solve problems when it says this there are times when even though samples are correct and description matches samples it says samples dont match and then goes on to fail the problem on sample case. Also why sample test feeding helps could be because sample cases could help alert models to making some trivial error either while setting up brute force or coming up with an answer. (2122 B, gpt)

Also 2123 D is interesting because it only matches outputs upto samples that were visible the first sample that wasnt visible it made up an answer to, maybe more search for problems where it just brute matches a couple of samples and fails otherwise, suggesting logical dependence on samples. Also further research could be done on when it is good at identifying which samples are wrong or correct by giving it wrong samples?

Also CoT indicates a lot of basis on previous seen problems for gpt
Should ask about what to do regarding Binary String Game chat in chatgpt
